{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cap10_2_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEY7qEZdFgL7OJW7l5rYU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusrpb/116319_estruturasdedados/blob/main/cap10_2_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRV91hYd6U_h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmqrpmbD88Jk"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation,Dropout, Flatten, Embedding, SimpleRNN\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hv5Vo6uB_7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d680872-7e9d-435c-9d63-a4251d2ba1cc"
      },
      "source": [
        "max_words = 10000\n",
        "maxlen = 500\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2)\n",
        "#word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "2121728/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBk69_tpCJOU"
      },
      "source": [
        "#index_to_word = {}\n",
        "#for key, value in word_index.items():\n",
        "#    index_to_word[value] = key\n",
        "#print(' '.join([index_to_word[x] for x in x_train[0]]))\n",
        "#print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVKLX4OQh9Pc"
      },
      "source": [
        "Next, we need to truncate and pad the input sequences so that they are all the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfTvh0HdCPLb",
        "outputId": "83fb0f74-96c2-4e03-ee09-56a194650f1f"
      },
      "source": [
        "num_classes = max(y_train) + 1\n",
        "\n",
        "#tokenizer = Tokenizer(num_words=max_words)\n",
        "#X_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "#X_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "X_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "print(X_train[0])\n",
        "print(len(X_train[0]))\n",
        "print(X_train.shape)\n",
        "\n",
        "print(y_train[0])\n",
        "#print(len(y_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    1    2    2    8   43   10  447\n",
            "    5   25  207  270    5 3095  111   16  369  186   90   67    7   89\n",
            "    5   19  102    6   19  124   15   90   67   84   22  482   26    7\n",
            "   48    4   49    8  864   39  209  154    6  151    6   83   11   15\n",
            "   22  155   11   15    7   48    9 4579 1005  504    6  258    6  272\n",
            "   11   15   22  134   44   11   15   16    8  197 1245   90   67   52\n",
            "   29  209   30   32  132    6  109   15   17   12]\n",
            "500\n",
            "(8982, 500)\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiyHXjCRnwo7"
      },
      "source": [
        "The second use is in the Natural Language Processing and its related applications whre we have to create the word embeddings for all the words present in the documents of our corpus.\n",
        "\n",
        "Thus the embedding layer in Keras can be used when we want to create the embeddings to embed higher dimensional data into lower dimensional vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9nLOVrZhxo9"
      },
      "source": [
        "Own data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Gmb4SejBK6"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKrMvAn2hzIl"
      },
      "source": [
        "test_set = pd.read_csv(\"/content/2016train.csv\",sep=',')\n",
        "train_set = pd.read_csv(\"/content/2016test.csv\",sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrSSUAS5PYlN"
      },
      "source": [
        "test_set['pol'] = pd.Categorical(test_set['pol'])\n",
        "\n",
        "test_set['pol'] = test_set['pol'].cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jvu9qyWPjus"
      },
      "source": [
        "train_set['pol'] = pd.Categorical(train_set['pol'])\n",
        "\n",
        "train_set['pol'] = train_set['pol'].cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "aroNlVs-lSTa",
        "outputId": "8320c58d-25cc-4fea-cb76-d2d5982192f7"
      },
      "source": [
        "test_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pol</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@Microsoft how about you make a system that do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>I may be ignorant on this issue but... should ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Thanks to @microsoft I just may be switching o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>If I make a game as a #windows10 Universal App...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>2</td>\n",
              "      <td>@Racalto_SK ok good to know. Punting at MetLif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1</td>\n",
              "      <td>everyone who sat around me at metlife was so a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1</td>\n",
              "      <td>what giants or niners fans would wanna go to t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>2</td>\n",
              "      <td>Anybody want a ticket for tomorrow Colombia vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>1</td>\n",
              "      <td>Mendez told me he'd drive me to MetLife on Sun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      pol                                               text\n",
              "0       0  dear @Microsoft the newOoffice for Mac is grea...\n",
              "1       0  @Microsoft how about you make a system that do...\n",
              "2       0  I may be ignorant on this issue but... should ...\n",
              "3       0  Thanks to @microsoft I just may be switching o...\n",
              "4       1  If I make a game as a #windows10 Universal App...\n",
              "...   ...                                                ...\n",
              "5995    2  @Racalto_SK ok good to know. Punting at MetLif...\n",
              "5996    1  everyone who sat around me at metlife was so a...\n",
              "5997    1  what giants or niners fans would wanna go to t...\n",
              "5998    2  Anybody want a ticket for tomorrow Colombia vs...\n",
              "5999    1  Mendez told me he'd drive me to MetLife on Sun...\n",
              "\n",
              "[6000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejt2DYwqnsx_"
      },
      "source": [
        "y_train = train_set['pol']\n",
        "X_train = train_set['text']\n",
        "\n",
        "y_test = test_set['pol']\n",
        "X_test = test_set['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SySgfaseoszi"
      },
      "source": [
        "num_classes = 3\n",
        "\n",
        "from collections import Counter\n",
        "results = Counter()\n",
        "train_set['text'].str.lower().str.split().apply(results.update)\n",
        "test_set['text'].str.lower().str.split().apply(results.update)\n",
        "total_vocab_size = len(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzTG9bOxPUuF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5QBpuenljvA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLAgN_DWozxX"
      },
      "source": [
        "Tokenizacao e vetorizacao simples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExkpAWwuoVkK",
        "outputId": "6b419ee5-a065-485e-c373-fb3c0d76ca57"
      },
      "source": [
        "import nltk\n",
        "from keras.preprocessing.text import one_hot,Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_list_nltk = nltk.corpus.stopwords.words('english')\n",
        "# tokenizing\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "\n",
        "corp=X_train\n",
        "corp2=X_test\n",
        "no_docs=len(corp)\n",
        "no_docs2=len(corp2)\n",
        "print(no_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "20632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1p9EsJMrQLv"
      },
      "source": [
        "vocab_size=40\n",
        "encod_corp=[]\n",
        "for i,doc in enumerate(corp):\n",
        "    encod_corp.append(one_hot(doc,50))\n",
        "    #print(\"Document \"+str(i)+doc)\n",
        "    #print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))\n",
        "\n",
        "vocab_size=40\n",
        "encod_corp2=[]\n",
        "for i,doc in enumerate(corp2):\n",
        "    encod_corp2.append(one_hot(doc,50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTv3jDrNre54",
        "outputId": "e9ab06f4-1938-40e0-c544-2ab23e89bfe7"
      },
      "source": [
        "# length of maximum document. will be nedded whenever create embeddings for the words\n",
        "maxlen=-1\n",
        "nltk.download('punkt')\n",
        "for doc in corp:\n",
        "    tokens=nltk.word_tokenize(doc)\n",
        "    if(maxlen<len(tokens)):\n",
        "        maxlen=len(tokens)\n",
        "print(\"The maximum number of words in any document is : \",maxlen)\n",
        "\n",
        "# length of maximum document. will be nedded whenever create embeddings for the words\n",
        "maxlen2=-1\n",
        "nltk.download('punkt')\n",
        "for doc2 in corp2:\n",
        "    tokens=nltk.word_tokenize(doc2)\n",
        "    if(maxlen2<len(tokens)):\n",
        "        maxlen2=len(tokens)\n",
        "print(\"The maximum number of words in any document is : \",maxlen2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "The maximum number of words in any document is :  53\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "The maximum number of words in any document is :  52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs9jvH0W3YCF"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X_train_new=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\n",
        "X_test_new=pad_sequences(encod_corp2,maxlen=maxlen2,padding='post',value=0.0)\n",
        "\n",
        "#print(\"No of padded documents: \",len(pad_corp))\n",
        "\n",
        "#for i,doc in enumerate(pad_corp):\n",
        "     #print(\"The padded encoding for document\",i+1,\" is : \",doc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC97CGEsR378"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_new, y_train, test_size=0.2, random_state=42,stratify=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlsDSj_BNs5i"
      },
      "source": [
        "\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_valid = to_categorical(y_valid)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwqkU-I2o0dP"
      },
      "source": [
        "tokenização por word2vec:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98wH5hNKo2in"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrK-fOU0oNJr"
      },
      "source": [
        "#X_train = sequence.pad_sequences(X_train, maxlen=50)\n",
        "#X_test = sequence.pad_sequences(X_test, maxlen=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghwMEMjbndQV",
        "outputId": "bd384a6f-28e1-4088-bca4-57921c448cef"
      },
      "source": [
        "#X_train[0]\n",
        "pad_corp[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33, 13, 46, 36, 35, 21, 28,  9, 32, 18, 24, 45,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAbZSOhQ5DBt",
        "outputId": "c559b121-15d7-46d5-87da-9860f15b5a85"
      },
      "source": [
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "embedding_vector_length=32\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_vocab_size, embedding_vector_length, input_length=maxlen))\n",
        "model.add(LSTM(100))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(4))\n",
        "model.add(Activation('softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 53, 32)            2043616   \n",
            "_________________________________________________________________\n",
            "lstm_19 (LSTM)               (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 4)                 404       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 2,097,220\n",
            "Trainable params: 2,097,220\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1JCfmKboXYC",
        "outputId": "09881657-a9a6-4c1d-e491-bfb3c43558a4"
      },
      "source": [
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "embedding_vector_features=50\n",
        "\n",
        "model=Sequential()\n",
        "\n",
        "model.add(Embedding(vocab_size,embedding_vector_features,input_length=50))\n",
        "\n",
        "model.add(LSTM(128,return_sequences=True))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(128,return_sequences=True))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# for units in [128,128,64,32]:\n",
        "\n",
        "# model.add(Dense(units,activation='relu'))\n",
        "\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32,activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(num_classes,activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 50, 50)            2000      \n",
            "_________________________________________________________________\n",
            "lstm_20 (LSTM)               (None, 50, 128)           91648     \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 229,459\n",
            "Trainable params: 229,459\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mumdFsrp45Ef"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "sgd = SGD(learning_rate=0.1)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_h4fxEO5bBd",
        "outputId": "7746658a-9a56-4a9b-b83f-186b3a7cd9e0"
      },
      "source": [
        "model.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.float32, name='embedding_14_input'), name='embedding_14_input', description=\"created by layer 'embedding_14_input'\"), but it was called on an input with incompatible shape (None, 53).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.float32, name='embedding_14_input'), name='embedding_14_input', description=\"created by layer 'embedding_14_input'\"), but it was called on an input with incompatible shape (None, 53).\n",
            "516/516 [==============================] - ETA: 0s - loss: 1.0074 - accuracy: 0.5006WARNING:tensorflow:Model was constructed with shape (None, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.float32, name='embedding_14_input'), name='embedding_14_input', description=\"created by layer 'embedding_14_input'\"), but it was called on an input with incompatible shape (None, 53).\n",
            "516/516 [==============================] - 91s 165ms/step - loss: 1.0074 - accuracy: 0.5006 - val_loss: 1.0061 - val_accuracy: 0.5013\n",
            "Epoch 2/10\n",
            "516/516 [==============================] - 84s 163ms/step - loss: 1.0042 - accuracy: 0.5012 - val_loss: 1.0048 - val_accuracy: 0.5013\n",
            "Epoch 3/10\n",
            "516/516 [==============================] - 85s 164ms/step - loss: 1.0051 - accuracy: 0.5012 - val_loss: 1.0036 - val_accuracy: 0.5013\n",
            "Epoch 4/10\n",
            "516/516 [==============================] - 87s 168ms/step - loss: 1.0043 - accuracy: 0.5012 - val_loss: 1.0068 - val_accuracy: 0.5013\n",
            "Epoch 5/10\n",
            "516/516 [==============================] - 86s 167ms/step - loss: 1.0045 - accuracy: 0.5012 - val_loss: 1.0034 - val_accuracy: 0.5013\n",
            "Epoch 6/10\n",
            "516/516 [==============================] - 87s 169ms/step - loss: 1.0041 - accuracy: 0.5012 - val_loss: 1.0037 - val_accuracy: 0.5013\n",
            "Epoch 7/10\n",
            "516/516 [==============================] - 90s 175ms/step - loss: 1.0041 - accuracy: 0.5012 - val_loss: 1.0035 - val_accuracy: 0.5013\n",
            "Epoch 8/10\n",
            "516/516 [==============================] - 86s 167ms/step - loss: 1.0042 - accuracy: 0.5012 - val_loss: 1.0035 - val_accuracy: 0.5013\n",
            "Epoch 9/10\n",
            "516/516 [==============================] - 90s 174ms/step - loss: 1.0040 - accuracy: 0.5012 - val_loss: 1.0051 - val_accuracy: 0.5013\n",
            "Epoch 10/10\n",
            "516/516 [==============================] - 87s 169ms/step - loss: 1.0039 - accuracy: 0.5012 - val_loss: 1.0037 - val_accuracy: 0.5013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f38cf4084d0>"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWSNKy858Qns"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53gvQbFppsc8"
      },
      "source": [
        "word2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8ehewkNptxT"
      },
      "source": [
        "import gensim.downloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DAFdRgfpzDC",
        "outputId": "241aca1a-2aad-428c-dee2-e72fd2c671d5"
      },
      "source": [
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhYfmk8kp2YL",
        "outputId": "f879634e-3ad2-4b2c-d56c-9dd3e3905cb9"
      },
      "source": [
        "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZBFKv4GqFEi",
        "outputId": "93fee8d5-7721-4f3d-eb0c-22ce6350c770"
      },
      "source": [
        "word2vec_vectors.most_similar('ice')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ice', 0.6578792333602905),\n",
              " ('Francies_tossed', 0.605116605758667),\n",
              " ('Melting_polar', 0.5964572429656982),\n",
              " ('melting_glacial', 0.5638922452926636),\n",
              " ('icy', 0.5635038614273071),\n",
              " ('Fill_cocktail_shaker', 0.5591110587120056),\n",
              " ('caked_oak_tree', 0.5552520155906677),\n",
              " ('ice_melts', 0.5514487028121948),\n",
              " ('snow', 0.5391556024551392),\n",
              " ('cocktail_shaker_filled', 0.5355455875396729)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvihutqpsoJB",
        "outputId": "de3f0ed6-00eb-40f3-9b09-517931f9c76c"
      },
      "source": [
        "vector = word2vec_vectors['office']\n",
        "vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbvGGkq8YvrH"
      },
      "source": [
        "BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OubdCATWYwun"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}